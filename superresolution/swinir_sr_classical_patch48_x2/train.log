25-10-01 09:43:41.302 :   task: swinir_sr_classical_patch48_x2
  model: plain
  gpu_ids: [0]
  scale: 2
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    task: superresolution\swinir_sr_classical_patch48_x2
    log: superresolution\swinir_sr_classical_patch48_x2
    options: superresolution\swinir_sr_classical_patch48_x2\options
    models: superresolution\swinir_sr_classical_patch48_x2\models
    images: superresolution\swinir_sr_classical_patch48_x2\images
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: ../datasets/DIV2K/DIV2K_train_HR
      dataroot_L: ../datasets/DIV2K/DIV2K_train_LR_bicubic/X2
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 4
      dataloader_batch_size: 8
      phase: train
      scale: 2
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: ../datasets/DIV2K/DIV2K_valid_HR
      dataroot_L: ../datasets/DIV2K/DIV2K_valid_LR_bicubic/X2
      phase: test
      scale: 2
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 2500
    checkpoint_print: 10
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: False
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

25-10-01 09:43:41.313 : Number of train images: 800, iters: 100
25-10-01 09:43:49.082 : 
Networks name: SwinIR
Params number: 11752487
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

25-10-01 09:43:49.320 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.001 | -0.192 |  0.192 |  0.110 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 |  0.018 | -0.191 |  0.192 |  0.108 | torch.Size([180]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.065 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.088 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.073 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.084 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.096 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.056 |  0.075 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.081 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.086 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.087 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.060 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.085 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.077 |  0.095 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -2.000 |  0.091 |  0.021 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.085 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.072 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.090 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.085 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.093 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.091 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.062 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.094 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.079 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.086 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.095 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.058 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.089 |  0.103 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.084 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.100 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.089 |  0.077 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.060 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.080 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.085 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.075 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.090 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.059 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.088 |  0.098 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.083 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.083 |  0.100 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.078 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.091 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.086 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.097 |  0.098 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.089 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.079 |  0.076 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.085 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.081 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.078 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.078 |  0.093 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.063 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.083 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.083 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.094 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.070 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.080 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.100 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 |  0.001 | -0.025 |  0.024 |  0.013 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.085 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.082 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.073 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.097 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.100 |  0.077 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.069 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.083 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.078 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.083 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.060 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.094 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.073 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.077 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.084 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.068 |  0.075 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.086 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.084 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.088 |  0.097 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.060 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.084 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.061 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.096 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.080 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.084 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.000 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.095 |  0.055 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.093 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.076 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.083 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.084 |  0.095 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.067 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.095 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.081 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.090 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.084 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.056 |  0.053 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.095 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.081 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.083 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.082 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.062 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.084 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.092 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.088 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.087 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.062 |  0.083 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.087 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.088 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.088 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.083 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.068 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.092 |  0.106 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.074 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.082 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.103 |  0.097 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.069 |  0.058 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.085 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.079 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.086 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.086 |  0.102 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.085 |  0.053 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.084 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.082 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.093 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.082 |  0.099 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.066 |  0.055 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.088 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.088 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.102 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.067 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.083 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.089 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.093 |  0.077 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.093 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.069 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.097 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.077 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.092 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.082 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.056 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.087 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.083 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.087 |  0.076 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 | -0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.072 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.096 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.096 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.086 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.094 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.069 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.091 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.092 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.081 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.090 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.067 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.094 |  0.098 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.082 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.085 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.088 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.057 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.084 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.074 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.087 |  0.096 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.084 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.065 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.084 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.076 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.085 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.078 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.070 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.089 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.077 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.082 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.085 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.000 | -0.024 |  0.024 |  0.014 | torch.Size([180]) || layers.5.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || norm.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.002 | -0.025 |  0.024 |  0.014 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.002 | -0.024 |  0.024 |  0.015 | torch.Size([64]) || conv_before_upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.002 | -0.042 |  0.041 |  0.024 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 | -0.008 | -0.039 |  0.023 |  0.031 | torch.Size([3]) || conv_last.bias

25-10-01 09:44:04.851 : <epoch:  0, iter:      10, lr:2.000e-04> G_loss: 9.575e-02 
25-10-01 09:44:08.207 : <epoch:  0, iter:      20, lr:2.000e-04> G_loss: 9.686e-02 
25-10-01 09:44:11.562 : <epoch:  0, iter:      30, lr:2.000e-04> G_loss: 8.972e-02 
25-10-01 09:44:14.953 : <epoch:  0, iter:      40, lr:2.000e-04> G_loss: 5.826e-02 
25-10-01 09:44:18.336 : <epoch:  0, iter:      50, lr:2.000e-04> G_loss: 1.091e-01 
25-10-01 09:44:21.699 : <epoch:  0, iter:      60, lr:2.000e-04> G_loss: 7.747e-02 
25-10-01 09:44:25.075 : <epoch:  0, iter:      70, lr:2.000e-04> G_loss: 8.080e-02 
25-10-01 09:44:28.452 : <epoch:  0, iter:      80, lr:2.000e-04> G_loss: 7.471e-02 
25-10-01 09:44:31.826 : <epoch:  0, iter:      90, lr:2.000e-04> G_loss: 6.019e-02 
25-10-01 09:44:35.209 : <epoch:  0, iter:     100, lr:2.000e-04> G_loss: 4.745e-02 
25-10-01 09:44:53.260 : <epoch:  1, iter:     110, lr:2.000e-04> G_loss: 6.749e-02 
25-10-01 09:44:56.642 : <epoch:  1, iter:     120, lr:2.000e-04> G_loss: 5.936e-02 
25-10-01 09:45:00.026 : <epoch:  1, iter:     130, lr:2.000e-04> G_loss: 5.290e-02 
25-10-01 09:45:03.420 : <epoch:  1, iter:     140, lr:2.000e-04> G_loss: 6.187e-02 
25-10-01 09:45:06.812 : <epoch:  1, iter:     150, lr:2.000e-04> G_loss: 5.580e-02 
25-10-01 09:45:10.206 : <epoch:  1, iter:     160, lr:2.000e-04> G_loss: 6.905e-02 
25-10-01 09:45:13.596 : <epoch:  1, iter:     170, lr:2.000e-04> G_loss: 5.834e-02 
25-10-01 09:45:16.991 : <epoch:  1, iter:     180, lr:2.000e-04> G_loss: 5.917e-02 
25-10-01 09:45:20.390 : <epoch:  1, iter:     190, lr:2.000e-04> G_loss: 7.130e-02 
25-10-01 09:45:23.780 : <epoch:  1, iter:     200, lr:2.000e-04> G_loss: 5.450e-02 
25-10-01 09:45:38.659 : <epoch:  2, iter:     210, lr:2.000e-04> G_loss: 4.588e-02 
25-10-01 09:45:42.069 : <epoch:  2, iter:     220, lr:2.000e-04> G_loss: 5.027e-02 
25-10-01 09:45:45.461 : <epoch:  2, iter:     230, lr:2.000e-04> G_loss: 4.828e-02 
25-10-01 09:45:48.856 : <epoch:  2, iter:     240, lr:2.000e-04> G_loss: 4.567e-02 
25-10-01 09:45:52.257 : <epoch:  2, iter:     250, lr:2.000e-04> G_loss: 8.038e-02 
25-10-01 09:45:55.660 : <epoch:  2, iter:     260, lr:2.000e-04> G_loss: 4.222e-02 
25-10-01 09:45:59.053 : <epoch:  2, iter:     270, lr:2.000e-04> G_loss: 4.927e-02 
25-10-01 09:46:02.453 : <epoch:  2, iter:     280, lr:2.000e-04> G_loss: 5.981e-02 
25-10-01 09:46:05.858 : <epoch:  2, iter:     290, lr:2.000e-04> G_loss: 5.622e-02 
25-10-01 09:46:09.252 : <epoch:  2, iter:     300, lr:2.000e-04> G_loss: 5.713e-02 
25-10-01 09:46:24.038 : <epoch:  3, iter:     310, lr:2.000e-04> G_loss: 5.911e-02 
25-10-01 09:46:27.432 : <epoch:  3, iter:     320, lr:2.000e-04> G_loss: 5.054e-02 
25-10-01 09:46:30.817 : <epoch:  3, iter:     330, lr:2.000e-04> G_loss: 4.271e-02 
25-10-01 09:46:34.216 : <epoch:  3, iter:     340, lr:2.000e-04> G_loss: 3.533e-02 
25-10-01 09:46:37.612 : <epoch:  3, iter:     350, lr:2.000e-04> G_loss: 5.495e-02 
25-10-01 09:46:41.015 : <epoch:  3, iter:     360, lr:2.000e-04> G_loss: 4.167e-02 
25-10-01 09:46:44.419 : <epoch:  3, iter:     370, lr:2.000e-04> G_loss: 4.521e-02 
25-10-01 09:46:47.838 : <epoch:  3, iter:     380, lr:2.000e-04> G_loss: 5.555e-02 
25-10-01 09:46:51.275 : <epoch:  3, iter:     390, lr:2.000e-04> G_loss: 3.273e-02 
25-10-01 09:46:54.694 : <epoch:  3, iter:     400, lr:2.000e-04> G_loss: 4.258e-02 
25-10-01 09:47:09.444 : <epoch:  4, iter:     410, lr:2.000e-04> G_loss: 4.003e-02 
25-10-01 09:47:12.841 : <epoch:  4, iter:     420, lr:2.000e-04> G_loss: 2.931e-02 
25-10-01 09:47:16.230 : <epoch:  4, iter:     430, lr:2.000e-04> G_loss: 3.473e-02 
25-10-01 09:47:19.635 : <epoch:  4, iter:     440, lr:2.000e-04> G_loss: 4.796e-02 
25-10-01 09:47:23.030 : <epoch:  4, iter:     450, lr:2.000e-04> G_loss: 3.426e-02 
25-10-01 09:47:26.432 : <epoch:  4, iter:     460, lr:2.000e-04> G_loss: 2.504e-02 
25-10-01 09:47:29.851 : <epoch:  4, iter:     470, lr:2.000e-04> G_loss: 3.824e-02 
25-10-01 09:47:33.277 : <epoch:  4, iter:     480, lr:2.000e-04> G_loss: 3.255e-02 
25-10-01 09:47:36.710 : <epoch:  4, iter:     490, lr:2.000e-04> G_loss: 4.925e-02 
25-10-01 09:47:40.142 : <epoch:  4, iter:     500, lr:2.000e-04> G_loss: 2.827e-02 
25-10-01 09:47:55.056 : <epoch:  5, iter:     510, lr:2.000e-04> G_loss: 3.735e-02 
25-10-01 09:47:58.465 : <epoch:  5, iter:     520, lr:2.000e-04> G_loss: 3.608e-02 
25-10-01 09:48:01.862 : <epoch:  5, iter:     530, lr:2.000e-04> G_loss: 3.039e-02 
25-10-01 09:48:05.263 : <epoch:  5, iter:     540, lr:2.000e-04> G_loss: 3.134e-02 
25-10-01 09:48:08.676 : <epoch:  5, iter:     550, lr:2.000e-04> G_loss: 5.366e-02 
25-10-01 09:48:12.082 : <epoch:  5, iter:     560, lr:2.000e-04> G_loss: 4.009e-02 
25-10-01 09:48:15.499 : <epoch:  5, iter:     570, lr:2.000e-04> G_loss: 4.341e-02 
25-10-01 09:48:18.944 : <epoch:  5, iter:     580, lr:2.000e-04> G_loss: 4.190e-02 
25-10-01 09:48:22.392 : <epoch:  5, iter:     590, lr:2.000e-04> G_loss: 4.133e-02 
25-10-01 09:48:25.833 : <epoch:  5, iter:     600, lr:2.000e-04> G_loss: 3.255e-02 
25-10-01 09:48:40.642 : <epoch:  6, iter:     610, lr:2.000e-04> G_loss: 3.916e-02 
25-10-01 09:48:44.040 : <epoch:  6, iter:     620, lr:2.000e-04> G_loss: 2.799e-02 
25-10-01 09:48:47.428 : <epoch:  6, iter:     630, lr:2.000e-04> G_loss: 3.617e-02 
25-10-01 09:48:50.831 : <epoch:  6, iter:     640, lr:2.000e-04> G_loss: 3.131e-02 
25-10-01 09:48:54.237 : <epoch:  6, iter:     650, lr:2.000e-04> G_loss: 3.870e-02 
25-10-01 09:48:57.657 : <epoch:  6, iter:     660, lr:2.000e-04> G_loss: 2.994e-02 
25-10-01 09:49:01.074 : <epoch:  6, iter:     670, lr:2.000e-04> G_loss: 2.824e-02 
25-10-01 09:49:04.514 : <epoch:  6, iter:     680, lr:2.000e-04> G_loss: 2.604e-02 
25-10-01 09:49:07.958 : <epoch:  6, iter:     690, lr:2.000e-04> G_loss: 2.761e-02 
25-10-01 09:49:11.385 : <epoch:  6, iter:     700, lr:2.000e-04> G_loss: 3.003e-02 
25-10-01 09:49:26.068 : <epoch:  7, iter:     710, lr:2.000e-04> G_loss: 2.451e-02 
25-10-01 09:49:29.463 : <epoch:  7, iter:     720, lr:2.000e-04> G_loss: 2.782e-02 
25-10-01 09:49:32.858 : <epoch:  7, iter:     730, lr:2.000e-04> G_loss: 1.458e-02 
25-10-01 09:49:36.254 : <epoch:  7, iter:     740, lr:2.000e-04> G_loss: 3.017e-02 
25-10-01 09:49:39.661 : <epoch:  7, iter:     750, lr:2.000e-04> G_loss: 2.862e-02 
25-10-01 09:49:43.073 : <epoch:  7, iter:     760, lr:2.000e-04> G_loss: 2.859e-02 
25-10-01 09:49:46.503 : <epoch:  7, iter:     770, lr:2.000e-04> G_loss: 1.819e-02 
25-10-01 09:49:49.943 : <epoch:  7, iter:     780, lr:2.000e-04> G_loss: 3.249e-02 
25-10-01 09:49:53.395 : <epoch:  7, iter:     790, lr:2.000e-04> G_loss: 2.332e-02 
25-10-01 09:49:56.844 : <epoch:  7, iter:     800, lr:2.000e-04> G_loss: 3.821e-02 
25-10-01 09:50:13.986 : <epoch:  8, iter:     810, lr:2.000e-04> G_loss: 3.379e-02 
25-10-01 09:50:17.413 : <epoch:  8, iter:     820, lr:2.000e-04> G_loss: 3.806e-02 
25-10-01 09:50:20.835 : <epoch:  8, iter:     830, lr:2.000e-04> G_loss: 2.538e-02 
25-10-01 09:50:24.250 : <epoch:  8, iter:     840, lr:2.000e-04> G_loss: 2.448e-02 
25-10-01 09:50:27.672 : <epoch:  8, iter:     850, lr:2.000e-04> G_loss: 4.536e-02 
25-10-01 09:50:31.085 : <epoch:  8, iter:     860, lr:2.000e-04> G_loss: 4.276e-02 
25-10-01 09:50:34.520 : <epoch:  8, iter:     870, lr:2.000e-04> G_loss: 3.219e-02 
25-10-01 09:50:37.966 : <epoch:  8, iter:     880, lr:2.000e-04> G_loss: 2.946e-02 
25-10-01 09:50:41.423 : <epoch:  8, iter:     890, lr:2.000e-04> G_loss: 2.828e-02 
25-10-01 09:50:44.864 : <epoch:  8, iter:     900, lr:2.000e-04> G_loss: 1.474e-02 
25-10-01 09:50:59.704 : <epoch:  9, iter:     910, lr:2.000e-04> G_loss: 3.596e-02 
25-10-01 09:51:03.105 : <epoch:  9, iter:     920, lr:2.000e-04> G_loss: 2.500e-02 
25-10-01 09:51:06.523 : <epoch:  9, iter:     930, lr:2.000e-04> G_loss: 3.800e-02 
25-10-01 09:51:09.938 : <epoch:  9, iter:     940, lr:2.000e-04> G_loss: 2.774e-02 
25-10-01 09:51:13.360 : <epoch:  9, iter:     950, lr:2.000e-04> G_loss: 3.860e-02 
25-10-01 09:51:16.780 : <epoch:  9, iter:     960, lr:2.000e-04> G_loss: 3.603e-02 
25-10-01 09:51:20.208 : <epoch:  9, iter:     970, lr:2.000e-04> G_loss: 3.239e-02 
25-10-01 09:51:23.657 : <epoch:  9, iter:     980, lr:2.000e-04> G_loss: 2.406e-02 
25-10-01 09:51:27.121 : <epoch:  9, iter:     990, lr:2.000e-04> G_loss: 3.081e-02 
25-10-01 09:51:30.571 : <epoch:  9, iter:   1,000, lr:2.000e-04> G_loss: 4.617e-02 
25-10-01 09:51:45.313 : <epoch: 10, iter:   1,010, lr:2.000e-04> G_loss: 3.098e-02 
25-10-01 09:51:48.709 : <epoch: 10, iter:   1,020, lr:2.000e-04> G_loss: 2.267e-02 
25-10-01 09:51:52.119 : <epoch: 10, iter:   1,030, lr:2.000e-04> G_loss: 3.536e-02 
25-10-01 09:51:55.523 : <epoch: 10, iter:   1,040, lr:2.000e-04> G_loss: 2.135e-02 
25-10-01 09:51:58.937 : <epoch: 10, iter:   1,050, lr:2.000e-04> G_loss: 4.884e-02 
25-10-01 09:52:02.370 : <epoch: 10, iter:   1,060, lr:2.000e-04> G_loss: 2.618e-02 
25-10-01 09:52:05.814 : <epoch: 10, iter:   1,070, lr:2.000e-04> G_loss: 3.209e-02 
25-10-01 09:52:09.261 : <epoch: 10, iter:   1,080, lr:2.000e-04> G_loss: 3.282e-02 
25-10-01 09:52:12.722 : <epoch: 10, iter:   1,090, lr:2.000e-04> G_loss: 2.902e-02 
25-10-01 09:52:16.175 : <epoch: 10, iter:   1,100, lr:2.000e-04> G_loss: 3.035e-02 
25-10-01 09:52:30.948 : <epoch: 11, iter:   1,110, lr:2.000e-04> G_loss: 3.469e-02 
25-10-01 09:52:34.348 : <epoch: 11, iter:   1,120, lr:2.000e-04> G_loss: 3.587e-02 
25-10-01 09:52:37.740 : <epoch: 11, iter:   1,130, lr:2.000e-04> G_loss: 2.689e-02 
25-10-01 09:52:41.140 : <epoch: 11, iter:   1,140, lr:2.000e-04> G_loss: 2.049e-02 
25-10-01 09:52:44.545 : <epoch: 11, iter:   1,150, lr:2.000e-04> G_loss: 2.689e-02 
25-10-01 09:52:47.969 : <epoch: 11, iter:   1,160, lr:2.000e-04> G_loss: 1.945e-02 
25-10-01 09:52:51.405 : <epoch: 11, iter:   1,170, lr:2.000e-04> G_loss: 2.531e-02 
25-10-01 09:52:54.847 : <epoch: 11, iter:   1,180, lr:2.000e-04> G_loss: 3.007e-02 
25-10-01 09:52:58.310 : <epoch: 11, iter:   1,190, lr:2.000e-04> G_loss: 3.606e-02 
25-10-01 09:53:01.755 : <epoch: 11, iter:   1,200, lr:2.000e-04> G_loss: 2.977e-02 
25-10-01 09:53:16.543 : <epoch: 12, iter:   1,210, lr:2.000e-04> G_loss: 2.814e-02 
25-10-01 09:53:19.944 : <epoch: 12, iter:   1,220, lr:2.000e-04> G_loss: 1.846e-02 
25-10-01 09:53:23.346 : <epoch: 12, iter:   1,230, lr:2.000e-04> G_loss: 3.013e-02 
25-10-01 09:53:26.751 : <epoch: 12, iter:   1,240, lr:2.000e-04> G_loss: 2.796e-02 
25-10-01 09:53:30.155 : <epoch: 12, iter:   1,250, lr:2.000e-04> G_loss: 1.807e-02 
25-10-01 09:53:33.582 : <epoch: 12, iter:   1,260, lr:2.000e-04> G_loss: 2.354e-02 
25-10-01 09:53:37.018 : <epoch: 12, iter:   1,270, lr:2.000e-04> G_loss: 2.489e-02 
25-10-01 09:53:40.473 : <epoch: 12, iter:   1,280, lr:2.000e-04> G_loss: 1.776e-02 
25-10-01 09:53:43.928 : <epoch: 12, iter:   1,290, lr:2.000e-04> G_loss: 2.162e-02 
25-10-01 09:53:47.387 : <epoch: 12, iter:   1,300, lr:2.000e-04> G_loss: 2.095e-02 
25-10-01 09:54:02.164 : <epoch: 13, iter:   1,310, lr:2.000e-04> G_loss: 2.804e-02 
25-10-01 09:54:05.572 : <epoch: 13, iter:   1,320, lr:2.000e-04> G_loss: 3.221e-02 
25-10-01 09:54:08.965 : <epoch: 13, iter:   1,330, lr:2.000e-04> G_loss: 3.253e-02 
25-10-01 09:54:12.369 : <epoch: 13, iter:   1,340, lr:2.000e-04> G_loss: 2.637e-02 
25-10-01 09:54:15.779 : <epoch: 13, iter:   1,350, lr:2.000e-04> G_loss: 1.936e-02 
25-10-01 09:54:19.211 : <epoch: 13, iter:   1,360, lr:2.000e-04> G_loss: 2.586e-02 
25-10-01 09:54:22.661 : <epoch: 13, iter:   1,370, lr:2.000e-04> G_loss: 2.743e-02 
25-10-01 09:54:26.122 : <epoch: 13, iter:   1,380, lr:2.000e-04> G_loss: 2.519e-02 
25-10-01 09:54:29.595 : <epoch: 13, iter:   1,390, lr:2.000e-04> G_loss: 3.605e-02 
25-10-01 09:54:33.044 : <epoch: 13, iter:   1,400, lr:2.000e-04> G_loss: 3.708e-02 
25-10-01 09:54:47.996 : <epoch: 14, iter:   1,410, lr:2.000e-04> G_loss: 5.093e-02 
25-10-01 09:54:51.391 : <epoch: 14, iter:   1,420, lr:2.000e-04> G_loss: 3.566e-02 
25-10-01 09:54:54.811 : <epoch: 14, iter:   1,430, lr:2.000e-04> G_loss: 1.769e-02 
25-10-01 09:54:58.226 : <epoch: 14, iter:   1,440, lr:2.000e-04> G_loss: 2.618e-02 
25-10-01 09:55:01.645 : <epoch: 14, iter:   1,450, lr:2.000e-04> G_loss: 3.037e-02 
25-10-01 09:55:05.064 : <epoch: 14, iter:   1,460, lr:2.000e-04> G_loss: 2.539e-02 
25-10-01 09:55:08.511 : <epoch: 14, iter:   1,470, lr:2.000e-04> G_loss: 3.636e-02 
25-10-01 09:55:11.960 : <epoch: 14, iter:   1,480, lr:2.000e-04> G_loss: 3.437e-02 
25-10-01 09:55:15.427 : <epoch: 14, iter:   1,490, lr:2.000e-04> G_loss: 1.739e-02 
25-10-01 09:55:18.892 : <epoch: 14, iter:   1,500, lr:2.000e-04> G_loss: 2.938e-02 
25-10-01 09:55:34.000 : <epoch: 15, iter:   1,510, lr:2.000e-04> G_loss: 2.423e-02 
25-10-01 09:55:37.409 : <epoch: 15, iter:   1,520, lr:2.000e-04> G_loss: 2.270e-02 
25-10-01 09:55:40.852 : <epoch: 15, iter:   1,530, lr:2.000e-04> G_loss: 2.339e-02 
25-10-01 09:55:45.452 : <epoch: 15, iter:   1,540, lr:2.000e-04> G_loss: 2.584e-02 
25-10-01 09:55:48.941 : <epoch: 15, iter:   1,550, lr:2.000e-04> G_loss: 2.092e-02 
25-10-01 09:55:52.411 : <epoch: 15, iter:   1,560, lr:2.000e-04> G_loss: 2.125e-02 
25-10-01 09:55:55.879 : <epoch: 15, iter:   1,570, lr:2.000e-04> G_loss: 2.265e-02 
25-10-01 09:55:59.349 : <epoch: 15, iter:   1,580, lr:2.000e-04> G_loss: 2.120e-02 
25-10-01 09:56:02.813 : <epoch: 15, iter:   1,590, lr:2.000e-04> G_loss: 4.008e-02 
25-10-01 09:56:06.267 : <epoch: 15, iter:   1,600, lr:2.000e-04> G_loss: 2.015e-02 
25-10-01 09:56:21.394 : <epoch: 16, iter:   1,610, lr:2.000e-04> G_loss: 2.261e-02 
25-10-01 09:56:24.810 : <epoch: 16, iter:   1,620, lr:2.000e-04> G_loss: 1.305e-02 
25-10-01 09:56:28.226 : <epoch: 16, iter:   1,630, lr:2.000e-04> G_loss: 2.232e-02 
25-10-01 09:56:31.650 : <epoch: 16, iter:   1,640, lr:2.000e-04> G_loss: 2.281e-02 
25-10-01 09:56:35.071 : <epoch: 16, iter:   1,650, lr:2.000e-04> G_loss: 3.060e-02 
25-10-01 09:56:38.507 : <epoch: 16, iter:   1,660, lr:2.000e-04> G_loss: 2.050e-02 
25-10-01 09:56:41.954 : <epoch: 16, iter:   1,670, lr:2.000e-04> G_loss: 2.381e-02 
25-10-01 09:56:45.421 : <epoch: 16, iter:   1,680, lr:2.000e-04> G_loss: 2.717e-02 
25-10-01 09:56:48.898 : <epoch: 16, iter:   1,690, lr:2.000e-04> G_loss: 1.923e-02 
25-10-01 09:56:52.359 : <epoch: 16, iter:   1,700, lr:2.000e-04> G_loss: 1.704e-02 
25-10-01 09:57:08.009 : <epoch: 17, iter:   1,710, lr:2.000e-04> G_loss: 2.373e-02 
25-10-01 09:57:11.417 : <epoch: 17, iter:   1,720, lr:2.000e-04> G_loss: 2.542e-02 
25-10-01 09:57:14.848 : <epoch: 17, iter:   1,730, lr:2.000e-04> G_loss: 2.410e-02 
25-10-01 09:57:18.261 : <epoch: 17, iter:   1,740, lr:2.000e-04> G_loss: 1.880e-02 
25-10-01 09:57:21.674 : <epoch: 17, iter:   1,750, lr:2.000e-04> G_loss: 3.376e-02 
25-10-01 09:57:25.079 : <epoch: 17, iter:   1,760, lr:2.000e-04> G_loss: 2.279e-02 
25-10-01 09:57:28.520 : <epoch: 17, iter:   1,770, lr:2.000e-04> G_loss: 1.842e-02 
25-10-01 09:57:31.965 : <epoch: 17, iter:   1,780, lr:2.000e-04> G_loss: 3.186e-02 
25-10-01 09:57:35.416 : <epoch: 17, iter:   1,790, lr:2.000e-04> G_loss: 3.452e-02 
25-10-01 09:57:38.872 : <epoch: 17, iter:   1,800, lr:2.000e-04> G_loss: 3.327e-02 
25-10-01 09:57:53.803 : <epoch: 18, iter:   1,810, lr:2.000e-04> G_loss: 2.734e-02 
25-10-01 09:57:57.197 : <epoch: 18, iter:   1,820, lr:2.000e-04> G_loss: 3.537e-02 
25-10-01 09:58:00.602 : <epoch: 18, iter:   1,830, lr:2.000e-04> G_loss: 1.638e-02 
25-10-01 09:58:04.007 : <epoch: 18, iter:   1,840, lr:2.000e-04> G_loss: 2.406e-02 
25-10-01 09:58:07.402 : <epoch: 18, iter:   1,850, lr:2.000e-04> G_loss: 3.376e-02 
25-10-01 09:58:10.812 : <epoch: 18, iter:   1,860, lr:2.000e-04> G_loss: 3.977e-02 
25-10-01 09:58:14.243 : <epoch: 18, iter:   1,870, lr:2.000e-04> G_loss: 2.033e-02 
25-10-01 09:58:17.684 : <epoch: 18, iter:   1,880, lr:2.000e-04> G_loss: 2.382e-02 
25-10-01 09:58:21.137 : <epoch: 18, iter:   1,890, lr:2.000e-04> G_loss: 2.494e-02 
25-10-01 09:58:24.585 : <epoch: 18, iter:   1,900, lr:2.000e-04> G_loss: 4.470e-02 
25-10-01 09:58:39.419 : <epoch: 19, iter:   1,910, lr:2.000e-04> G_loss: 2.771e-02 
25-10-01 09:58:42.816 : <epoch: 19, iter:   1,920, lr:2.000e-04> G_loss: 1.729e-02 
25-10-01 09:58:46.212 : <epoch: 19, iter:   1,930, lr:2.000e-04> G_loss: 2.612e-02 
25-10-01 09:58:49.613 : <epoch: 19, iter:   1,940, lr:2.000e-04> G_loss: 1.650e-02 
25-10-01 09:58:53.016 : <epoch: 19, iter:   1,950, lr:2.000e-04> G_loss: 2.428e-02 
25-10-01 09:58:56.429 : <epoch: 19, iter:   1,960, lr:2.000e-04> G_loss: 2.170e-02 
25-10-01 09:58:59.873 : <epoch: 19, iter:   1,970, lr:2.000e-04> G_loss: 2.539e-02 
25-10-01 09:59:03.327 : <epoch: 19, iter:   1,980, lr:2.000e-04> G_loss: 2.139e-02 
25-10-01 09:59:06.797 : <epoch: 19, iter:   1,990, lr:2.000e-04> G_loss: 2.322e-02 
25-10-01 09:59:10.246 : <epoch: 19, iter:   2,000, lr:2.000e-04> G_loss: 1.818e-02 
25-10-01 09:59:24.822 : <epoch: 20, iter:   2,010, lr:2.000e-04> G_loss: 1.910e-02 
25-10-01 09:59:28.218 : <epoch: 20, iter:   2,020, lr:2.000e-04> G_loss: 2.148e-02 
25-10-01 09:59:31.635 : <epoch: 20, iter:   2,030, lr:2.000e-04> G_loss: 2.301e-02 
25-10-01 09:59:35.040 : <epoch: 20, iter:   2,040, lr:2.000e-04> G_loss: 2.318e-02 
25-10-01 09:59:38.446 : <epoch: 20, iter:   2,050, lr:2.000e-04> G_loss: 2.534e-02 
25-10-01 09:59:41.871 : <epoch: 20, iter:   2,060, lr:2.000e-04> G_loss: 2.612e-02 
25-10-01 09:59:45.305 : <epoch: 20, iter:   2,070, lr:2.000e-04> G_loss: 2.250e-02 
25-10-01 09:59:48.751 : <epoch: 20, iter:   2,080, lr:2.000e-04> G_loss: 2.480e-02 
25-10-01 09:59:52.207 : <epoch: 20, iter:   2,090, lr:2.000e-04> G_loss: 2.282e-02 
25-10-01 09:59:55.666 : <epoch: 20, iter:   2,100, lr:2.000e-04> G_loss: 2.281e-02 
25-10-01 10:00:10.221 : <epoch: 21, iter:   2,110, lr:2.000e-04> G_loss: 2.506e-02 
25-10-01 10:00:13.608 : <epoch: 21, iter:   2,120, lr:2.000e-04> G_loss: 1.440e-02 
25-10-01 10:00:17.007 : <epoch: 21, iter:   2,130, lr:2.000e-04> G_loss: 2.713e-02 
25-10-01 10:00:20.412 : <epoch: 21, iter:   2,140, lr:2.000e-04> G_loss: 1.994e-02 
25-10-01 10:00:23.826 : <epoch: 21, iter:   2,150, lr:2.000e-04> G_loss: 2.955e-02 
25-10-01 10:00:27.254 : <epoch: 21, iter:   2,160, lr:2.000e-04> G_loss: 2.116e-02 
25-10-01 10:00:30.702 : <epoch: 21, iter:   2,170, lr:2.000e-04> G_loss: 3.475e-02 
25-10-01 10:00:34.152 : <epoch: 21, iter:   2,180, lr:2.000e-04> G_loss: 3.374e-02 
25-10-01 10:00:37.617 : <epoch: 21, iter:   2,190, lr:2.000e-04> G_loss: 3.521e-02 
25-10-01 10:00:41.070 : <epoch: 21, iter:   2,200, lr:2.000e-04> G_loss: 2.011e-02 
25-10-01 10:00:55.620 : <epoch: 22, iter:   2,210, lr:2.000e-04> G_loss: 4.291e-02 
25-10-01 10:00:59.021 : <epoch: 22, iter:   2,220, lr:2.000e-04> G_loss: 2.212e-02 
25-10-01 10:01:02.429 : <epoch: 22, iter:   2,230, lr:2.000e-04> G_loss: 2.073e-02 
25-10-01 10:01:05.847 : <epoch: 22, iter:   2,240, lr:2.000e-04> G_loss: 2.205e-02 
25-10-01 10:01:09.269 : <epoch: 22, iter:   2,250, lr:2.000e-04> G_loss: 2.219e-02 
25-10-01 10:01:12.733 : <epoch: 22, iter:   2,260, lr:2.000e-04> G_loss: 1.653e-02 
25-10-01 10:01:17.762 : <epoch: 22, iter:   2,270, lr:2.000e-04> G_loss: 2.786e-02 
25-10-01 10:01:21.212 : <epoch: 22, iter:   2,280, lr:2.000e-04> G_loss: 2.134e-02 
25-10-01 10:01:24.685 : <epoch: 22, iter:   2,290, lr:2.000e-04> G_loss: 2.150e-02 
25-10-01 10:01:28.141 : <epoch: 22, iter:   2,300, lr:2.000e-04> G_loss: 2.406e-02 
25-10-01 10:01:43.046 : <epoch: 23, iter:   2,310, lr:2.000e-04> G_loss: 2.093e-02 
25-10-01 10:01:46.440 : <epoch: 23, iter:   2,320, lr:2.000e-04> G_loss: 2.769e-02 
25-10-01 10:01:49.848 : <epoch: 23, iter:   2,330, lr:2.000e-04> G_loss: 2.477e-02 
25-10-01 10:01:53.267 : <epoch: 23, iter:   2,340, lr:2.000e-04> G_loss: 2.288e-02 
25-10-01 10:01:56.680 : <epoch: 23, iter:   2,350, lr:2.000e-04> G_loss: 2.445e-02 
25-10-01 10:02:00.109 : <epoch: 23, iter:   2,360, lr:2.000e-04> G_loss: 1.985e-02 
25-10-01 10:02:03.548 : <epoch: 23, iter:   2,370, lr:2.000e-04> G_loss: 2.419e-02 
25-10-01 10:02:07.009 : <epoch: 23, iter:   2,380, lr:2.000e-04> G_loss: 1.636e-02 
25-10-01 10:02:10.478 : <epoch: 23, iter:   2,390, lr:2.000e-04> G_loss: 3.171e-02 
25-10-01 10:02:13.946 : <epoch: 23, iter:   2,400, lr:2.000e-04> G_loss: 2.403e-02 
25-10-01 10:02:28.857 : <epoch: 24, iter:   2,410, lr:2.000e-04> G_loss: 1.701e-02 
25-10-01 10:02:32.251 : <epoch: 24, iter:   2,420, lr:2.000e-04> G_loss: 2.967e-02 
25-10-01 10:02:35.653 : <epoch: 24, iter:   2,430, lr:2.000e-04> G_loss: 2.690e-02 
25-10-01 10:02:39.057 : <epoch: 24, iter:   2,440, lr:2.000e-04> G_loss: 2.020e-02 
25-10-01 10:02:42.460 : <epoch: 24, iter:   2,450, lr:2.000e-04> G_loss: 2.062e-02 
25-10-01 10:02:45.881 : <epoch: 24, iter:   2,460, lr:2.000e-04> G_loss: 3.143e-02 
25-10-01 10:02:49.322 : <epoch: 24, iter:   2,470, lr:2.000e-04> G_loss: 2.886e-02 
25-10-01 10:02:52.780 : <epoch: 24, iter:   2,480, lr:2.000e-04> G_loss: 1.200e-02 
25-10-01 10:02:56.235 : <epoch: 24, iter:   2,490, lr:2.000e-04> G_loss: 2.329e-02 
25-10-01 10:02:59.681 : <epoch: 24, iter:   2,500, lr:2.000e-04> G_loss: 1.642e-02 
25-10-01 10:02:59.681 : Saving the model.
25-10-01 10:03:15.891 : <epoch: 25, iter:   2,510, lr:2.000e-04> G_loss: 2.210e-02 
25-10-01 10:03:19.287 : <epoch: 25, iter:   2,520, lr:2.000e-04> G_loss: 1.966e-02 
25-10-01 10:03:22.697 : <epoch: 25, iter:   2,530, lr:2.000e-04> G_loss: 2.020e-02 
25-10-01 10:03:26.104 : <epoch: 25, iter:   2,540, lr:2.000e-04> G_loss: 3.330e-02 
25-10-01 10:03:29.507 : <epoch: 25, iter:   2,550, lr:2.000e-04> G_loss: 1.323e-02 
25-10-01 10:03:32.938 : <epoch: 25, iter:   2,560, lr:2.000e-04> G_loss: 1.415e-02 
25-10-01 10:03:36.372 : <epoch: 25, iter:   2,570, lr:2.000e-04> G_loss: 2.354e-02 
25-10-01 10:03:39.821 : <epoch: 25, iter:   2,580, lr:2.000e-04> G_loss: 2.565e-02 
25-10-01 10:03:43.278 : <epoch: 25, iter:   2,590, lr:2.000e-04> G_loss: 1.723e-02 
25-10-01 10:03:46.726 : <epoch: 25, iter:   2,600, lr:2.000e-04> G_loss: 2.063e-02 
25-10-01 10:04:01.485 : <epoch: 26, iter:   2,610, lr:2.000e-04> G_loss: 2.940e-02 
25-10-01 10:04:04.895 : <epoch: 26, iter:   2,620, lr:2.000e-04> G_loss: 2.318e-02 
25-10-01 10:04:08.302 : <epoch: 26, iter:   2,630, lr:2.000e-04> G_loss: 2.062e-02 
25-10-01 10:04:11.703 : <epoch: 26, iter:   2,640, lr:2.000e-04> G_loss: 1.767e-02 
25-10-01 10:04:15.107 : <epoch: 26, iter:   2,650, lr:2.000e-04> G_loss: 1.904e-02 
25-10-01 10:04:18.532 : <epoch: 26, iter:   2,660, lr:2.000e-04> G_loss: 1.353e-02 
25-10-01 10:04:21.981 : <epoch: 26, iter:   2,670, lr:2.000e-04> G_loss: 1.608e-02 
25-10-01 10:04:25.441 : <epoch: 26, iter:   2,680, lr:2.000e-04> G_loss: 3.119e-02 
25-10-01 10:04:28.897 : <epoch: 26, iter:   2,690, lr:2.000e-04> G_loss: 2.257e-02 
25-10-01 10:04:32.358 : <epoch: 26, iter:   2,700, lr:2.000e-04> G_loss: 2.016e-02 
25-10-01 10:04:47.652 : <epoch: 27, iter:   2,710, lr:2.000e-04> G_loss: 2.952e-02 
25-10-01 10:04:51.405 : <epoch: 27, iter:   2,720, lr:2.000e-04> G_loss: 3.238e-02 
25-10-01 10:04:54.970 : <epoch: 27, iter:   2,730, lr:2.000e-04> G_loss: 2.276e-02 
25-10-01 10:04:58.424 : <epoch: 27, iter:   2,740, lr:2.000e-04> G_loss: 2.397e-02 
25-10-01 10:05:01.862 : <epoch: 27, iter:   2,750, lr:2.000e-04> G_loss: 1.985e-02 
25-10-01 10:05:05.288 : <epoch: 27, iter:   2,760, lr:2.000e-04> G_loss: 2.085e-02 
25-10-01 10:05:08.740 : <epoch: 27, iter:   2,770, lr:2.000e-04> G_loss: 1.925e-02 
25-10-01 10:05:12.180 : <epoch: 27, iter:   2,780, lr:2.000e-04> G_loss: 2.418e-02 
25-10-01 10:05:15.636 : <epoch: 27, iter:   2,790, lr:2.000e-04> G_loss: 2.579e-02 
25-10-01 10:05:19.115 : <epoch: 27, iter:   2,800, lr:2.000e-04> G_loss: 1.589e-02 
25-10-01 10:05:34.172 : <epoch: 28, iter:   2,810, lr:2.000e-04> G_loss: 2.932e-02 
25-10-01 10:05:37.587 : <epoch: 28, iter:   2,820, lr:2.000e-04> G_loss: 1.967e-02 
25-10-01 10:05:41.048 : <epoch: 28, iter:   2,830, lr:2.000e-04> G_loss: 3.034e-02 
25-10-01 10:05:44.507 : <epoch: 28, iter:   2,840, lr:2.000e-04> G_loss: 1.915e-02 
25-10-01 10:05:47.938 : <epoch: 28, iter:   2,850, lr:2.000e-04> G_loss: 1.724e-02 
25-10-01 10:05:51.374 : <epoch: 28, iter:   2,860, lr:2.000e-04> G_loss: 1.591e-02 
25-10-01 10:05:54.874 : <epoch: 28, iter:   2,870, lr:2.000e-04> G_loss: 1.973e-02 
25-10-01 10:05:58.357 : <epoch: 28, iter:   2,880, lr:2.000e-04> G_loss: 1.219e-02 
25-10-01 10:06:01.857 : <epoch: 28, iter:   2,890, lr:2.000e-04> G_loss: 1.811e-02 
25-10-01 10:06:05.342 : <epoch: 28, iter:   2,900, lr:2.000e-04> G_loss: 1.684e-02 
25-10-01 10:06:20.032 : <epoch: 29, iter:   2,910, lr:2.000e-04> G_loss: 2.185e-02 
25-10-01 10:06:23.446 : <epoch: 29, iter:   2,920, lr:2.000e-04> G_loss: 2.300e-02 
25-10-01 10:06:26.860 : <epoch: 29, iter:   2,930, lr:2.000e-04> G_loss: 3.082e-02 
25-10-01 10:06:30.269 : <epoch: 29, iter:   2,940, lr:2.000e-04> G_loss: 1.534e-02 
25-10-01 10:06:33.711 : <epoch: 29, iter:   2,950, lr:2.000e-04> G_loss: 1.949e-02 
25-10-01 10:06:37.177 : <epoch: 29, iter:   2,960, lr:2.000e-04> G_loss: 2.656e-02 
25-10-01 10:06:40.686 : <epoch: 29, iter:   2,970, lr:2.000e-04> G_loss: 2.069e-02 
25-10-01 10:06:44.223 : <epoch: 29, iter:   2,980, lr:2.000e-04> G_loss: 1.786e-02 
25-10-01 10:06:49.532 : <epoch: 29, iter:   2,990, lr:2.000e-04> G_loss: 3.388e-02 
25-10-01 10:06:53.003 : <epoch: 29, iter:   3,000, lr:2.000e-04> G_loss: 2.365e-02 
25-10-01 10:07:08.242 : <epoch: 30, iter:   3,010, lr:2.000e-04> G_loss: 2.382e-02 
25-10-01 10:07:11.686 : <epoch: 30, iter:   3,020, lr:2.000e-04> G_loss: 2.312e-02 
25-10-01 10:07:15.108 : <epoch: 30, iter:   3,030, lr:2.000e-04> G_loss: 2.587e-02 
25-10-01 10:07:18.539 : <epoch: 30, iter:   3,040, lr:2.000e-04> G_loss: 2.784e-02 
25-10-01 10:07:21.991 : <epoch: 30, iter:   3,050, lr:2.000e-04> G_loss: 2.194e-02 
25-10-01 10:07:25.463 : <epoch: 30, iter:   3,060, lr:2.000e-04> G_loss: 2.585e-02 
25-10-01 10:07:28.941 : <epoch: 30, iter:   3,070, lr:2.000e-04> G_loss: 1.725e-02 
25-10-01 10:07:32.438 : <epoch: 30, iter:   3,080, lr:2.000e-04> G_loss: 2.097e-02 
25-10-01 10:07:35.906 : <epoch: 30, iter:   3,090, lr:2.000e-04> G_loss: 1.941e-02 
25-10-01 10:07:39.408 : <epoch: 30, iter:   3,100, lr:2.000e-04> G_loss: 2.671e-02 
25-10-01 10:07:54.490 : <epoch: 31, iter:   3,110, lr:2.000e-04> G_loss: 3.589e-02 
25-10-01 10:07:57.905 : <epoch: 31, iter:   3,120, lr:2.000e-04> G_loss: 2.093e-02 
25-10-01 10:08:01.333 : <epoch: 31, iter:   3,130, lr:2.000e-04> G_loss: 2.898e-02 
25-10-01 10:08:04.895 : <epoch: 31, iter:   3,140, lr:2.000e-04> G_loss: 1.943e-02 
25-10-01 10:08:08.395 : <epoch: 31, iter:   3,150, lr:2.000e-04> G_loss: 2.423e-02 
25-10-01 10:08:11.899 : <epoch: 31, iter:   3,160, lr:2.000e-04> G_loss: 2.419e-02 
25-10-01 10:08:15.404 : <epoch: 31, iter:   3,170, lr:2.000e-04> G_loss: 2.581e-02 
25-10-01 10:08:18.934 : <epoch: 31, iter:   3,180, lr:2.000e-04> G_loss: 1.338e-02 
25-10-01 10:08:22.414 : <epoch: 31, iter:   3,190, lr:2.000e-04> G_loss: 1.567e-02 
25-10-01 10:08:25.874 : <epoch: 31, iter:   3,200, lr:2.000e-04> G_loss: 2.024e-02 
25-10-01 10:08:40.803 : <epoch: 32, iter:   3,210, lr:2.000e-04> G_loss: 1.585e-02 
25-10-01 10:08:44.300 : <epoch: 32, iter:   3,220, lr:2.000e-04> G_loss: 1.845e-02 
25-10-01 10:08:47.705 : <epoch: 32, iter:   3,230, lr:2.000e-04> G_loss: 2.896e-02 
25-10-01 10:08:51.112 : <epoch: 32, iter:   3,240, lr:2.000e-04> G_loss: 2.381e-02 
25-10-01 10:08:54.520 : <epoch: 32, iter:   3,250, lr:2.000e-04> G_loss: 2.515e-02 
25-10-01 10:08:58.077 : <epoch: 32, iter:   3,260, lr:2.000e-04> G_loss: 2.369e-02 
25-10-01 10:09:01.705 : <epoch: 32, iter:   3,270, lr:2.000e-04> G_loss: 1.701e-02 
25-10-01 10:09:05.373 : <epoch: 32, iter:   3,280, lr:2.000e-04> G_loss: 2.029e-02 
25-10-01 10:09:09.059 : <epoch: 32, iter:   3,290, lr:2.000e-04> G_loss: 1.999e-02 
25-10-01 10:09:12.726 : <epoch: 32, iter:   3,300, lr:2.000e-04> G_loss: 1.842e-02 
25-10-01 10:09:27.789 : <epoch: 33, iter:   3,310, lr:2.000e-04> G_loss: 3.526e-02 
25-10-01 10:09:31.270 : <epoch: 33, iter:   3,320, lr:2.000e-04> G_loss: 2.490e-02 
25-10-01 10:09:34.751 : <epoch: 33, iter:   3,330, lr:2.000e-04> G_loss: 2.815e-02 
25-10-01 10:09:38.171 : <epoch: 33, iter:   3,340, lr:2.000e-04> G_loss: 3.738e-02 
25-10-01 10:09:41.610 : <epoch: 33, iter:   3,350, lr:2.000e-04> G_loss: 2.343e-02 
25-10-01 10:09:45.086 : <epoch: 33, iter:   3,360, lr:2.000e-04> G_loss: 2.151e-02 
25-10-01 10:09:48.549 : <epoch: 33, iter:   3,370, lr:2.000e-04> G_loss: 1.467e-02 
25-10-01 10:09:52.012 : <epoch: 33, iter:   3,380, lr:2.000e-04> G_loss: 4.048e-02 
25-10-01 10:09:55.533 : <epoch: 33, iter:   3,390, lr:2.000e-04> G_loss: 1.743e-02 
25-10-01 10:09:59.004 : <epoch: 33, iter:   3,400, lr:2.000e-04> G_loss: 1.334e-02 
25-10-01 10:10:14.046 : <epoch: 34, iter:   3,410, lr:2.000e-04> G_loss: 1.997e-02 
25-10-01 10:10:17.448 : <epoch: 34, iter:   3,420, lr:2.000e-04> G_loss: 2.185e-02 
25-10-01 10:10:20.978 : <epoch: 34, iter:   3,430, lr:2.000e-04> G_loss: 1.708e-02 
25-10-01 10:10:24.561 : <epoch: 34, iter:   3,440, lr:2.000e-04> G_loss: 1.809e-02 
25-10-01 10:10:28.050 : <epoch: 34, iter:   3,450, lr:2.000e-04> G_loss: 1.709e-02 
25-10-01 10:10:31.704 : <epoch: 34, iter:   3,460, lr:2.000e-04> G_loss: 3.097e-02 
25-10-01 10:10:35.214 : <epoch: 34, iter:   3,470, lr:2.000e-04> G_loss: 2.198e-02 
25-10-01 10:10:38.689 : <epoch: 34, iter:   3,480, lr:2.000e-04> G_loss: 2.050e-02 
25-10-01 10:10:42.153 : <epoch: 34, iter:   3,490, lr:2.000e-04> G_loss: 1.848e-02 
25-10-01 10:10:45.715 : <epoch: 34, iter:   3,500, lr:2.000e-04> G_loss: 1.604e-02 
25-10-01 10:11:00.582 : <epoch: 35, iter:   3,510, lr:2.000e-04> G_loss: 2.087e-02 
25-10-01 10:11:03.973 : <epoch: 35, iter:   3,520, lr:2.000e-04> G_loss: 1.869e-02 
25-10-01 10:11:07.517 : <epoch: 35, iter:   3,530, lr:2.000e-04> G_loss: 1.935e-02 
25-10-01 10:11:11.073 : <epoch: 35, iter:   3,540, lr:2.000e-04> G_loss: 1.943e-02 
25-10-01 10:11:14.487 : <epoch: 35, iter:   3,550, lr:2.000e-04> G_loss: 2.963e-02 
25-10-01 10:11:17.995 : <epoch: 35, iter:   3,560, lr:2.000e-04> G_loss: 1.546e-02 
25-10-01 10:11:21.572 : <epoch: 35, iter:   3,570, lr:2.000e-04> G_loss: 2.180e-02 
25-10-01 10:11:25.030 : <epoch: 35, iter:   3,580, lr:2.000e-04> G_loss: 2.072e-02 
25-10-01 10:11:28.497 : <epoch: 35, iter:   3,590, lr:2.000e-04> G_loss: 2.604e-02 
25-10-01 10:11:32.109 : <epoch: 35, iter:   3,600, lr:2.000e-04> G_loss: 1.145e-02 
25-10-01 10:11:47.228 : <epoch: 36, iter:   3,610, lr:2.000e-04> G_loss: 1.826e-02 
25-10-01 10:11:50.630 : <epoch: 36, iter:   3,620, lr:2.000e-04> G_loss: 2.337e-02 
25-10-01 10:11:54.129 : <epoch: 36, iter:   3,630, lr:2.000e-04> G_loss: 1.496e-02 
25-10-01 10:11:57.654 : <epoch: 36, iter:   3,640, lr:2.000e-04> G_loss: 2.662e-02 
25-10-01 10:12:01.164 : <epoch: 36, iter:   3,650, lr:2.000e-04> G_loss: 1.507e-02 
25-10-01 10:12:04.604 : <epoch: 36, iter:   3,660, lr:2.000e-04> G_loss: 1.154e-02 
25-10-01 10:12:08.058 : <epoch: 36, iter:   3,670, lr:2.000e-04> G_loss: 1.854e-02 
25-10-01 10:12:11.591 : <epoch: 36, iter:   3,680, lr:2.000e-04> G_loss: 1.605e-02 
25-10-01 10:12:15.174 : <epoch: 36, iter:   3,690, lr:2.000e-04> G_loss: 2.842e-02 
25-10-01 10:12:20.445 : <epoch: 36, iter:   3,700, lr:2.000e-04> G_loss: 1.687e-02 
25-10-01 10:12:35.730 : <epoch: 37, iter:   3,710, lr:2.000e-04> G_loss: 1.808e-02 
25-10-01 10:12:39.242 : <epoch: 37, iter:   3,720, lr:2.000e-04> G_loss: 2.063e-02 
25-10-01 10:12:43.110 : <epoch: 37, iter:   3,730, lr:2.000e-04> G_loss: 2.108e-02 
25-10-01 10:12:46.529 : <epoch: 37, iter:   3,740, lr:2.000e-04> G_loss: 2.872e-02 
25-10-01 10:12:49.957 : <epoch: 37, iter:   3,750, lr:2.000e-04> G_loss: 2.861e-02 
25-10-01 10:12:53.389 : <epoch: 37, iter:   3,760, lr:2.000e-04> G_loss: 1.772e-02 
25-10-01 10:12:56.940 : <epoch: 37, iter:   3,770, lr:2.000e-04> G_loss: 2.288e-02 
25-10-01 10:13:00.450 : <epoch: 37, iter:   3,780, lr:2.000e-04> G_loss: 2.119e-02 
25-10-01 10:13:03.933 : <epoch: 37, iter:   3,790, lr:2.000e-04> G_loss: 2.238e-02 
25-10-01 10:13:07.466 : <epoch: 37, iter:   3,800, lr:2.000e-04> G_loss: 1.718e-02 
25-10-01 10:13:22.536 : <epoch: 38, iter:   3,810, lr:2.000e-04> G_loss: 2.139e-02 
25-10-01 10:13:26.075 : <epoch: 38, iter:   3,820, lr:2.000e-04> G_loss: 2.768e-02 
25-10-01 10:13:29.565 : <epoch: 38, iter:   3,830, lr:2.000e-04> G_loss: 1.671e-02 
25-10-01 10:13:32.990 : <epoch: 38, iter:   3,840, lr:2.000e-04> G_loss: 1.432e-02 
25-10-01 10:13:36.429 : <epoch: 38, iter:   3,850, lr:2.000e-04> G_loss: 3.020e-02 
25-10-01 10:13:39.909 : <epoch: 38, iter:   3,860, lr:2.000e-04> G_loss: 1.754e-02 
25-10-01 10:13:43.388 : <epoch: 38, iter:   3,870, lr:2.000e-04> G_loss: 2.572e-02 
25-10-01 10:13:46.956 : <epoch: 38, iter:   3,880, lr:2.000e-04> G_loss: 2.225e-02 
25-10-01 10:13:50.514 : <epoch: 38, iter:   3,890, lr:2.000e-04> G_loss: 1.177e-02 
25-10-01 10:13:54.023 : <epoch: 38, iter:   3,900, lr:2.000e-04> G_loss: 3.713e-02 
25-10-01 10:14:09.006 : <epoch: 39, iter:   3,910, lr:2.000e-04> G_loss: 1.888e-02 
25-10-01 10:14:12.430 : <epoch: 39, iter:   3,920, lr:2.000e-04> G_loss: 2.403e-02 
25-10-01 10:14:15.849 : <epoch: 39, iter:   3,930, lr:2.000e-04> G_loss: 1.685e-02 
25-10-01 10:14:19.287 : <epoch: 39, iter:   3,940, lr:2.000e-04> G_loss: 2.397e-02 
25-10-01 10:14:22.729 : <epoch: 39, iter:   3,950, lr:2.000e-04> G_loss: 2.902e-02 
25-10-01 10:14:26.169 : <epoch: 39, iter:   3,960, lr:2.000e-04> G_loss: 2.381e-02 
25-10-01 10:14:29.642 : <epoch: 39, iter:   3,970, lr:2.000e-04> G_loss: 2.665e-02 
25-10-01 10:14:33.179 : <epoch: 39, iter:   3,980, lr:2.000e-04> G_loss: 2.688e-02 
25-10-01 10:14:36.731 : <epoch: 39, iter:   3,990, lr:2.000e-04> G_loss: 1.922e-02 
25-10-01 10:14:40.216 : <epoch: 39, iter:   4,000, lr:2.000e-04> G_loss: 2.697e-02 
25-10-01 10:14:55.475 : <epoch: 40, iter:   4,010, lr:2.000e-04> G_loss: 1.666e-02 
25-10-01 10:14:58.893 : <epoch: 40, iter:   4,020, lr:2.000e-04> G_loss: 1.616e-02 
25-10-01 10:15:02.343 : <epoch: 40, iter:   4,030, lr:2.000e-04> G_loss: 1.752e-02 
25-10-01 10:15:05.795 : <epoch: 40, iter:   4,040, lr:2.000e-04> G_loss: 1.839e-02 
25-10-01 10:15:09.247 : <epoch: 40, iter:   4,050, lr:2.000e-04> G_loss: 2.365e-02 
25-10-01 10:15:12.712 : <epoch: 40, iter:   4,060, lr:2.000e-04> G_loss: 1.667e-02 
25-10-01 10:15:16.188 : <epoch: 40, iter:   4,070, lr:2.000e-04> G_loss: 1.977e-02 
25-10-01 10:15:19.687 : <epoch: 40, iter:   4,080, lr:2.000e-04> G_loss: 2.473e-02 
25-10-01 10:15:23.203 : <epoch: 40, iter:   4,090, lr:2.000e-04> G_loss: 1.674e-02 
25-10-01 10:15:26.688 : <epoch: 40, iter:   4,100, lr:2.000e-04> G_loss: 1.868e-02 
25-10-01 10:15:41.623 : <epoch: 41, iter:   4,110, lr:2.000e-04> G_loss: 2.366e-02 
25-10-01 10:15:45.028 : <epoch: 41, iter:   4,120, lr:2.000e-04> G_loss: 1.522e-02 
25-10-01 10:15:48.444 : <epoch: 41, iter:   4,130, lr:2.000e-04> G_loss: 1.162e-02 
25-10-01 10:15:51.897 : <epoch: 41, iter:   4,140, lr:2.000e-04> G_loss: 1.415e-02 
25-10-01 10:15:55.362 : <epoch: 41, iter:   4,150, lr:2.000e-04> G_loss: 2.550e-02 
25-10-01 10:15:58.889 : <epoch: 41, iter:   4,160, lr:2.000e-04> G_loss: 1.704e-02 
25-10-01 10:16:02.436 : <epoch: 41, iter:   4,170, lr:2.000e-04> G_loss: 2.010e-02 
25-10-01 10:16:05.906 : <epoch: 41, iter:   4,180, lr:2.000e-04> G_loss: 2.652e-02 
25-10-01 10:16:09.512 : <epoch: 41, iter:   4,190, lr:2.000e-04> G_loss: 2.626e-02 
25-10-01 10:16:13.118 : <epoch: 41, iter:   4,200, lr:2.000e-04> G_loss: 1.632e-02 
25-10-01 10:16:28.332 : <epoch: 42, iter:   4,210, lr:2.000e-04> G_loss: 1.517e-02 
25-10-01 10:16:31.907 : <epoch: 42, iter:   4,220, lr:2.000e-04> G_loss: 1.803e-02 
25-10-01 10:16:35.564 : <epoch: 42, iter:   4,230, lr:2.000e-04> G_loss: 3.091e-02 
25-10-01 10:16:39.191 : <epoch: 42, iter:   4,240, lr:2.000e-04> G_loss: 1.565e-02 
25-10-01 10:16:42.720 : <epoch: 42, iter:   4,250, lr:2.000e-04> G_loss: 2.676e-02 
25-10-01 10:16:46.416 : <epoch: 42, iter:   4,260, lr:2.000e-04> G_loss: 1.877e-02 
25-10-01 10:16:50.052 : <epoch: 42, iter:   4,270, lr:2.000e-04> G_loss: 1.665e-02 
25-10-01 10:16:53.721 : <epoch: 42, iter:   4,280, lr:2.000e-04> G_loss: 1.291e-02 
25-10-01 10:16:57.446 : <epoch: 42, iter:   4,290, lr:2.000e-04> G_loss: 2.674e-02 
25-10-01 10:17:01.158 : <epoch: 42, iter:   4,300, lr:2.000e-04> G_loss: 1.395e-02 
25-10-01 10:17:16.346 : <epoch: 43, iter:   4,310, lr:2.000e-04> G_loss: 1.269e-02 
25-10-01 10:17:19.995 : <epoch: 43, iter:   4,320, lr:2.000e-04> G_loss: 2.117e-02 
25-10-01 10:17:23.642 : <epoch: 43, iter:   4,330, lr:2.000e-04> G_loss: 2.398e-02 
25-10-01 10:17:27.325 : <epoch: 43, iter:   4,340, lr:2.000e-04> G_loss: 1.943e-02 
25-10-01 10:17:30.977 : <epoch: 43, iter:   4,350, lr:2.000e-04> G_loss: 2.063e-02 
25-10-01 10:17:34.554 : <epoch: 43, iter:   4,360, lr:2.000e-04> G_loss: 2.794e-02 
25-10-01 10:17:38.175 : <epoch: 43, iter:   4,370, lr:2.000e-04> G_loss: 1.384e-02 
25-10-01 10:17:41.626 : <epoch: 43, iter:   4,380, lr:2.000e-04> G_loss: 2.108e-02 
25-10-01 10:17:45.387 : <epoch: 43, iter:   4,390, lr:2.000e-04> G_loss: 1.427e-02 
25-10-01 10:17:50.257 : <epoch: 43, iter:   4,400, lr:2.000e-04> G_loss: 2.069e-02 
25-10-01 10:18:06.294 : <epoch: 44, iter:   4,410, lr:2.000e-04> G_loss: 1.764e-02 
25-10-01 10:18:09.735 : <epoch: 44, iter:   4,420, lr:2.000e-04> G_loss: 1.549e-02 
25-10-01 10:18:13.219 : <epoch: 44, iter:   4,430, lr:2.000e-04> G_loss: 1.769e-02 
25-10-01 10:18:16.687 : <epoch: 44, iter:   4,440, lr:2.000e-04> G_loss: 2.813e-02 
25-10-01 10:18:20.139 : <epoch: 44, iter:   4,450, lr:2.000e-04> G_loss: 2.219e-02 
25-10-01 10:18:23.597 : <epoch: 44, iter:   4,460, lr:2.000e-04> G_loss: 1.554e-02 
25-10-01 10:18:27.081 : <epoch: 44, iter:   4,470, lr:2.000e-04> G_loss: 2.417e-02 
25-10-01 10:18:30.599 : <epoch: 44, iter:   4,480, lr:2.000e-04> G_loss: 2.967e-02 
25-10-01 10:18:34.101 : <epoch: 44, iter:   4,490, lr:2.000e-04> G_loss: 2.458e-02 
25-10-01 10:18:37.631 : <epoch: 44, iter:   4,500, lr:2.000e-04> G_loss: 1.851e-02 
25-10-01 10:18:52.685 : <epoch: 45, iter:   4,510, lr:2.000e-04> G_loss: 2.185e-02 
25-10-01 10:18:56.099 : <epoch: 45, iter:   4,520, lr:2.000e-04> G_loss: 2.111e-02 
25-10-01 10:18:59.523 : <epoch: 45, iter:   4,530, lr:2.000e-04> G_loss: 2.268e-02 
25-10-01 10:19:02.983 : <epoch: 45, iter:   4,540, lr:2.000e-04> G_loss: 2.769e-02 
25-10-01 10:19:06.436 : <epoch: 45, iter:   4,550, lr:2.000e-04> G_loss: 1.820e-02 
25-10-01 10:19:09.911 : <epoch: 45, iter:   4,560, lr:2.000e-04> G_loss: 2.265e-02 
25-10-01 10:19:13.362 : <epoch: 45, iter:   4,570, lr:2.000e-04> G_loss: 1.753e-02 
25-10-01 10:19:16.815 : <epoch: 45, iter:   4,580, lr:2.000e-04> G_loss: 3.716e-02 
25-10-01 10:19:20.286 : <epoch: 45, iter:   4,590, lr:2.000e-04> G_loss: 2.509e-02 
25-10-01 10:19:23.746 : <epoch: 45, iter:   4,600, lr:2.000e-04> G_loss: 2.346e-02 
25-10-01 10:19:38.677 : <epoch: 46, iter:   4,610, lr:2.000e-04> G_loss: 1.319e-02 
25-10-01 10:19:42.084 : <epoch: 46, iter:   4,620, lr:2.000e-04> G_loss: 2.960e-02 
25-10-01 10:19:45.488 : <epoch: 46, iter:   4,630, lr:2.000e-04> G_loss: 1.895e-02 
25-10-01 10:19:48.895 : <epoch: 46, iter:   4,640, lr:2.000e-04> G_loss: 2.681e-02 
25-10-01 10:19:52.308 : <epoch: 46, iter:   4,650, lr:2.000e-04> G_loss: 2.272e-02 
25-10-01 10:19:55.738 : <epoch: 46, iter:   4,660, lr:2.000e-04> G_loss: 1.701e-02 
25-10-01 10:19:59.185 : <epoch: 46, iter:   4,670, lr:2.000e-04> G_loss: 1.283e-02 
25-10-01 10:20:02.639 : <epoch: 46, iter:   4,680, lr:2.000e-04> G_loss: 2.659e-02 
25-10-01 10:20:06.107 : <epoch: 46, iter:   4,690, lr:2.000e-04> G_loss: 1.861e-02 
25-10-01 10:20:09.570 : <epoch: 46, iter:   4,700, lr:2.000e-04> G_loss: 1.406e-02 
25-10-01 10:20:24.297 : <epoch: 47, iter:   4,710, lr:2.000e-04> G_loss: 2.254e-02 
25-10-01 10:20:27.714 : <epoch: 47, iter:   4,720, lr:2.000e-04> G_loss: 2.765e-02 
25-10-01 10:20:31.119 : <epoch: 47, iter:   4,730, lr:2.000e-04> G_loss: 1.540e-02 
25-10-01 10:20:34.527 : <epoch: 47, iter:   4,740, lr:2.000e-04> G_loss: 1.254e-02 
25-10-01 10:20:37.938 : <epoch: 47, iter:   4,750, lr:2.000e-04> G_loss: 1.703e-02 
25-10-01 10:20:41.357 : <epoch: 47, iter:   4,760, lr:2.000e-04> G_loss: 1.372e-02 
25-10-01 10:20:44.816 : <epoch: 47, iter:   4,770, lr:2.000e-04> G_loss: 2.232e-02 
25-10-01 10:20:48.283 : <epoch: 47, iter:   4,780, lr:2.000e-04> G_loss: 1.933e-02 
25-10-01 10:20:51.753 : <epoch: 47, iter:   4,790, lr:2.000e-04> G_loss: 2.130e-02 
25-10-01 10:20:55.214 : <epoch: 47, iter:   4,800, lr:2.000e-04> G_loss: 1.771e-02 
25-10-01 10:21:10.091 : <epoch: 48, iter:   4,810, lr:2.000e-04> G_loss: 1.813e-02 
25-10-01 10:21:13.491 : <epoch: 48, iter:   4,820, lr:2.000e-04> G_loss: 1.735e-02 
25-10-01 10:21:16.901 : <epoch: 48, iter:   4,830, lr:2.000e-04> G_loss: 1.852e-02 
25-10-01 10:21:20.301 : <epoch: 48, iter:   4,840, lr:2.000e-04> G_loss: 1.381e-02 
25-10-01 10:21:23.713 : <epoch: 48, iter:   4,850, lr:2.000e-04> G_loss: 2.326e-02 
25-10-01 10:21:27.139 : <epoch: 48, iter:   4,860, lr:2.000e-04> G_loss: 1.409e-02 
25-10-01 10:21:30.585 : <epoch: 48, iter:   4,870, lr:2.000e-04> G_loss: 7.051e-03 
25-10-01 10:21:34.048 : <epoch: 48, iter:   4,880, lr:2.000e-04> G_loss: 1.628e-02 
25-10-01 10:21:37.514 : <epoch: 48, iter:   4,890, lr:2.000e-04> G_loss: 1.636e-02 
25-10-01 10:21:40.983 : <epoch: 48, iter:   4,900, lr:2.000e-04> G_loss: 2.384e-02 
25-10-01 10:21:55.702 : <epoch: 49, iter:   4,910, lr:2.000e-04> G_loss: 1.822e-02 
25-10-01 10:21:59.114 : <epoch: 49, iter:   4,920, lr:2.000e-04> G_loss: 2.149e-02 
25-10-01 10:22:02.520 : <epoch: 49, iter:   4,930, lr:2.000e-04> G_loss: 1.130e-02 
25-10-01 10:22:05.932 : <epoch: 49, iter:   4,940, lr:2.000e-04> G_loss: 2.338e-02 
25-10-01 10:22:09.352 : <epoch: 49, iter:   4,950, lr:2.000e-04> G_loss: 1.703e-02 
25-10-01 10:22:12.785 : <epoch: 49, iter:   4,960, lr:2.000e-04> G_loss: 2.145e-02 
25-10-01 10:22:16.228 : <epoch: 49, iter:   4,970, lr:2.000e-04> G_loss: 1.003e-02 
25-10-01 10:22:19.683 : <epoch: 49, iter:   4,980, lr:2.000e-04> G_loss: 2.096e-02 
25-10-01 10:22:23.151 : <epoch: 49, iter:   4,990, lr:2.000e-04> G_loss: 2.320e-02 
25-10-01 10:22:26.625 : <epoch: 49, iter:   5,000, lr:2.000e-04> G_loss: 1.794e-02 
25-10-01 10:22:26.625 : Saving the model.
25-10-01 10:22:39.864 : ---1-->   0801.png | 34.09dB
25-10-01 10:22:48.145 : ---2-->   0802.png | 36.87dB
